---
title: "Poisson Regression Examples"
author: "Sangho Lee"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
editor_options: 
  chunk_output_type: console
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

I will begin by acquiring and examining the dataset that is available. I will proceed to load and inspect this data using both Python and R, equipping me to perform various tasks as necessary.


```{r, include = FALSE}
if (!"reticulate" %in% installed.packages()) {
  install.packages("reticulate")
}

```

```{r, warning=FALSE, message=FALSE}
library(haven)
library(tidyverse)
library(magrittr)
library(scales)
library(data.table)
library(reticulate)

```



```{r}
blueprinty <- read.csv("blueprinty.csv")
blueprinty %>% 
  head(10) %>% 
  knitr::kable() 

```

```{python}
import pandas as pd
import numpy as np
import pyrsm as rsm
import matplotlib.pyplot as plt
import statsmodels.api as sm
import seaborn as sns

```

```{python}
blueprinty = pd.read_csv("blueprinty.csv")

```



<br><br>

Next, I will focus on comparing histograms and calculating the mean number of patents across different customer statuses. My aim is to observe any patterns or discrepancies that emerge from this comparison. By visualizing the distribution of patents with histograms, I will be able to gain insights into the skewness, modality, and spread of patent counts among customers.

In the subsequent analysis, attention should be directed towards:

- Constructing histograms for each category of customer status.
- Computing the mean number of patents for each customer status.
- Interpreting any noticeable trends or anomalies in the data.

I expect to see some information about how customer status might influence patent activity.

<br> 

```{python}
# Create separate dataframes for Blueprinty customers and non-customers
customers = blueprinty[blueprinty["iscustomer"] == 1]
non_customers = blueprinty[blueprinty["iscustomer"] == 0]

# Plot histograms with KDE to compare the distribution of patent counts
plt.figure(figsize=(16, 12))
sns.histplot(customers["patents"], kde=True, color='b', label='Customers', alpha=0.5)
sns.histplot(non_customers["patents"], kde=True, color='r', label='Non-customers', alpha=0.5)
plt.xlabel("Number of Patents", fontsize=18)
plt.ylabel("Frequency", fontsize=18)
plt.title("Histogram of Patents by Customer Status", fontsize=22)
plt.legend(fontsize=16)
plt.show()

```



```{python}
customers = blueprinty[blueprinty["iscustomer"] == 1]
non_customers = blueprinty[blueprinty["iscustomer"] == 0]

# Calculate the mean number of patents for customers and non-customers
mean_customers = customers["patents"].mean()
mean_non_customers = non_customers["patents"].mean()

mean_customers, mean_non_customers  

```


<br>

Based on the histogram with density and the mean calculations, here's what I observe:

##### Histogram Analysis
- Distribution Shape: The histogram shows that non-customers (in red) have a broader distribution, with the highest frequency between 2 and 5 patents. This group has a longer tail, extending up to 15 patents.
- Density: Customers (in blue) have a narrower distribution, with the peak density around 4 to 6 patents. This group has a lower overall frequency, suggesting fewer firms, but with a more concentrated range.
- Comparative Observation: Although non-customers have a broader distribution and more frequency in the lower range, customers tend to cluster in the mid-patent range, suggesting they may achieve higher patent counts on average.

##### Means
- Mean Values: The mean patent count for customers is 4.09, while for non-customers, it's 3.62.
- Interpretation: The higher mean for customers indicates that firms using Blueprinty's software tend to achieve more patents on average.

##### Overall
The data suggest that Blueprinty customers generally have a higher average patent count, with a tighter distribution around the 4-6 range. In contrast, non-customers display a broader distribution with a more substantial presence in the lower patent count range, but also a longer tail. This observation implies that using Blueprinty's software might correlate with achieving more patents on average. Further analysis could determine the statistical significance and factors contributing to these differences.



<br><br>

Now, I will compare the distribution of Blueprinty's customers across different regions, and their ages. I will find that certain regions have a higher concentration of customers, and there are specific age groups that are likely to be more prominent within our customer base. These patterns will suggest opportunities for targeted marketing campaigns and tailored service offerings based on regional and demographic trends.

```{python}
# Proportions of firms in each region by customer status
region_customer = customers["region"].value_counts(normalize=True)
region_non_customer = non_customers["region"].value_counts(normalize=True)

# Summary statistics for age by customer status
age_customer = customers["age"].describe()
age_non_customer = non_customers["age"].describe()

(region_customer, region_non_customer), (age_customer, age_non_customer) 


```


#### Regional Distribution
- Customers: A significant proportion of Blueprinty's customers are located in the Northeast (57.36%), followed by the Southwest (15.74%), South (10.15%), Midwest (8.63%), and Northwest (8.12%). This suggests that the Northeast region has a particularly high concentration of Blueprinty's customer base.
- Non-Customers: The regional distribution for non-customers is more evenly spread. The Northeast still leads (37.45%) but by a smaller margin compared to customers. The Southwest (20.41%), Midwest (15.89%), Northwest (13.12%), and South (13.12%) have a relatively more balanced distribution among non-customers.

#### Age Distribution
- Customers: The average age of Blueprinty's customer firms is 24.15 years, with a standard deviation of 7.56 years. The age range of customer firms spans from 10 to 46.5 years, with half of the customer base being 23 years old or younger.
Non-Customers: Non-customer firms have a slightly higher average age of 26.69 years and a standard deviation of 7.14 years, ranging from 9 to 49 years. The median age is 26.5 years, indicating that non-customer firms are generally older than customer firms.

#### Interpretation and Marketing Implications
- The Northeast is a key region for Blueprinty's customers, indicating a strong market presence or a higher demand for their software in this area. This could suggest potential for further market penetration or consolidation in the Northeast.
- The more balanced regional distribution of non-customers suggests there may be untapped potential in regions outside the Northeast, particularly in the Southwest and Midwest.
The age distribution shows that Blueprinty's customers tend to be slightly younger firms. This could imply that younger firms are more open to adopting new software for patent applications or that Blueprinty's offerings resonate more with such firms.
- Non-customers being slightly older could indicate market opportunities: either these firms are loyal to other products or they are not aware of the potential benefits of Blueprinty's software. This may point towards the need for outreach and education about Blueprinty's offerings, targeting older firms.



<br><br>

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.


Now, I'll write the code for the likelihood or log-likelihood function of the Poisson model. This will help me analyze count data effectively, which is essential for the project's statistical modeling. By understanding and implementing this function, I'll be better equipped to handle data that inherently follows a Poisson distribution.

```{r}
poisson_loglikelihood <- function(lambda, y) {
  n <- length(y)
  likelihood <- -n*lambda + sum(y*log(lambda)) - sum(lgamma(y + 1))
  return(likelihood)
}


```



Next, I will use the function I developed to plot the parameter lambda on the horizontal axis against the likelihood or log-likelihood values on the vertical axis. I'll vary lambda over a range and use the observed number of patents as the input value for Y. This visual representation will help me understand how the likelihood changes with different lambda values, aiding in selecting the most appropriate lambda for our model.

```{r}
y <- blueprinty$patents 

# Generate a sequence of lambda values
lambda_values <- seq(0.1, 10, by = 0.1) 

# Compute log-likelihood for each lambda
log_likelihoods <- sapply(lambda_values, function(l) poisson_loglikelihood(l, y))

# Plotting the log-likelihood against lambda
data_to_plot <- data.frame(lambda = lambda_values, log_likelihood = log_likelihoods)

ggplot(data_to_plot, aes(x = lambda, y = log_likelihood)) +
  geom_line() +
  labs(title = "Log-Likelihood of Poisson Model",
       x = "Lambda (\u03BB)",
       y = "Log-Likelihood") +
  theme_minimal()


```

#### Plot Interpretation
- Shape of the Curve: The curve is unimodal, displaying a clear peak where the log-likelihood is maximized. This peak corresponds to the value of 𝜆 that best fits the observed patent count data (in terms of maximizing the likelihood).

- Maximum Likelihood Estimate: The 𝜆 value at the peak of the curve is the Maximum Likelihood Estimate (MLE) for the Poisson model's parameter. This value of 𝜆 is where the slope of the curve is zero (the first derivative of the log-likelihood equals zero).

- Interpretation of 𝜆:
In the context of the Poisson distribution 𝜆 represents the average number of events (patent awards in this case) occurring in a fixed interval of time or space. The plot suggests that the MLE of 𝜆 is somewhere around 3, though the exact value isn't clear from the plot and would need to be found using the optim() function or by examining the computed log-likelihoods.

- Goodness of Fit: The width of the curve around the peak can give us an intuition about the precision of the estimate; a narrower peak would indicate a more precise estimate of 𝜆. However, to truly assess the goodness of fit, we would need to look at more than just the log-likelihood function, including other diagnostics such as residuals and goodness-of-fit tests.

- Usefulness: The log-likelihood plot is useful in optimization as it confirms that there is a maximum point that can be reached by adjusting 
𝜆, which is what the optim() function in R would seek to find.


I will now proceed to find the maximum likelihood estimate (MLE) by optimizing the likelihood function. I'll use the optim() function in R to perform this optimization. This will help identify the parameter values that make the observed data most probable under the assumed statistical model, ensuring the best fit for our analysis.


```{r}
# Define the negative log-likelihood function for the Poisson distribution
neg_log_likelihood_poisson <- function(lambda, y) {
  if (lambda <= 0) {
    return(Inf)  # Return a large number to avoid log of non-positive numbers
  }
  n <- length(y)
  likelihood <- n * lambda - sum(y * log(lambda)) + sum(lgamma(y + 1))
  return(likelihood)
}

# Assuming 'y' is your vector of observed patent counts
y <- blueprinty$patents  

# Use optim() to minimize the negative log-likelihood
# Starting value for lambda is chosen as the mean of y, which is a reasonable initial guess
result <- optim(par = mean(y), fn = neg_log_likelihood_poisson, y = y, method = "BFGS")

# The MLE of lambda
lambda_mle <- result$par

# Output the MLE of lambda
lambda_mle



```


The output of 3.684667 from the optim() function represents the maximum likelihood estimate (MLE) for the parameter𝜆 of the Poisson distribution, which models the number of patents awarded to each engineering firm over the last 5 years.

- Mean Rate (𝜆): The MLE 𝜆 of 3.684667 suggests that, under the Poisson model, the average number of patents awarded per firm over the specified period is approximately 3.68.

- Model Assumption: This estimation is based on the assumption that the number of patents awarded follows a Poisson distribution, where each count is independent of others and occurs with a constant mean rate.

- Data Fit: The estimated 𝜆 is the value that makes the observed data most probable under the Poisson model, meaning that it provides the best fit to the data according to the likelihood principle.

- In practical terms, this MLE can be used as an estimate for predicting the expected number of patents for a similar firm, assuming that no other information (like firm age, region, or customer status) is considered. It also serves as a baseline measure for comparisons in more complex models that include additional covariates.

<br>

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

Next, I'll enhance the flexibility and accuracy of our likelihood or log-likelihood function by including an additional argument to accommodate a covariate matrix, X. I will also adjust the model’s parameter from a single lambda to a beta vector. To ensure that lambda remains positive, as required, I’ll employ the exponential function as the inverse link function. Thus, we define lambda for each observation as $\lambda_i = e^{X_i' \beta}$. This modification allows us to model the influence of multiple covariates on our response variable, enhancing the robustness and relevance of our statistical analysis.

```{r}
poisson_regression_likelihood <- function(beta, Y, X) {
  lambda <- exp(X %*% beta)
  log_likelihood <- -sum(lambda) + sum(Y * log(lambda)) - sum(lgamma(Y + 1))
  return(-log_likelihood)
}
```

I will now utilize the updated likelihood or log-likelihood function, incorporating the covariate matrix X, to perform parameter estimation using R's optim(). The matrix X will be structured with its first column as all 1's to include a constant term in the model. Subsequent columns will include age, age squared, binary variables representing regions (excluding one to avoid multicollinearity), and a binary variable for customer status.

After determining the maximum likelihood estimate (MLE) vector, I'll use the Hessian matrix, returned by the optimization function, to calculate the standard errors of the beta parameter estimates. Finally, I will compile and present the results in a table format, showing both the coefficients and their corresponding standard errors. This approach will provide a comprehensive statistical basis for our model, ensuring precise interpretation and robust decision-making.

```{r}
### Prepare the Data
blueprinty$age_squared <- blueprinty$age^2  
blueprinty$region <- as.factor(blueprinty$region)  

# Create a model matrix including intercept, age, age squared, region, and iscustomer
# Dropping one region level to avoid dummy variable trap (reference category)
X <- model.matrix(~ age + age_squared + region + iscustomer, data = blueprinty)
Y <- blueprinty$patents

### Find the MlE using optim()
# Initial beta estimates
initial_beta <- rep(0, ncol(X))
result <- optim(par = initial_beta, fn = poisson_regression_likelihood, Y = Y, X = X, method = "BFGS", hessian = TRUE)
beta_mle <- result$par


### Compute Standard Errors Using the Hessian
hessian_matrix <- result$hessian

# Compute the covariance matrix by inverting the Hessian
covariance_matrix <- solve(hessian_matrix)

# Standard errors are the square roots of the diagonal elements of the covariance matrix
std_errors <- sqrt(diag(covariance_matrix))

# Create a table of coefficients and their standard errors
coef_table <- data.frame(Estimate = beta_mle, Std_Error = std_errors)
rownames(coef_table) <- colnames(X)

# Print the table
print(coef_table)


```

This lists the estimated coefficients from a Poisson regression model and their respective standard errors:

- Intercept (Constant term): The estimate is -0.2009 with a standard error of 0.1134. Since the Poisson model is multiplicative due to the log link function, the interpretation is not straightforward. However, this term represents the log count of patents when all other covariates are zero (which isn't necessarily a realistic scenario since age and age squared cannot be zero simultaneously).

- Age: The coefficient for age is 0.1187 with a standard error of 0.0064. This suggests that, holding all other variables constant, a one-year increase in the age of a firm is associated with an 11.87% increase in the expected log count of patents awarded.

- Age Squared: The age_squared term has a coefficient of -0.0022 with a standard error of 0.000077. The negative sign indicates that the relationship between age and patent count is not linear but rather parabolic, with a decreasing effect on patent count as age increases. This could suggest a peak age for firms at which the number of patents is maximized before it starts to decline.

- Region: The region coefficients compare each region to the base category, which is omitted to avoid multicollinearity (commonly the first alphabetical region). For instance, being in the Northeast is associated with an increase in the expected log count of patents by 0.0207 compared to the base region, but this is not statistically significant given the relatively large standard error.

- Is Customer: The coefficient for iscustomer is 0.0542 with a standard error of 0.0408. This implies that, all else being equal, firms that are customers of Blueprinty are expected to have a 5.42% increase in the log count of patents. The positive coefficient indicates a beneficial effect of using Blueprinty's software, but given the standard error, this result may not be statistically significant. To determine significance, one would typically look at the z-score (Estimate divided by Std_Error) and the corresponding p-value.

- In summary, the model suggests that the age of a firm and its squared term are important factors in predicting the success rate of patent awards, indicating a non-linear relationship. The coefficients for region suggest some regional effects, but the significance of these effects cannot be determined without additional statistics (such as p-values). Finally, being a Blueprinty customer appears to be associated with a higher expected count of patent awards, though without the p-values, we cannot confidently claim statistical significance.




_todo: Check your results using R's glm() function or Python sm.GLM() function._

```{r}
# Fit the Poisson regression model using glm()
poisson_glm <- glm(patents ~ age + I(age^2) + region + iscustomer, family = poisson(link = "log"), data = blueprinty)
summary(poisson_glm)

# Extract coefficients and their standard errors
coefficients_summary <- summary(poisson_glm)$coefficients

```


_todo: Interpret the results. What do you conclude about the effect of Blueprinty's software on patent success?_









